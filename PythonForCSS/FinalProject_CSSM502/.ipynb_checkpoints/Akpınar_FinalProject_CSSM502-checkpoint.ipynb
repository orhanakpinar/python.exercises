{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e16a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Preprocessing and Classification Score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Metric Printing to graph\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670c6d2",
   "metadata": {},
   "source": [
    "# Part1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacd2cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omen\\AppData\\Local\\Temp\\ipykernel_10436\\413832661.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  konda_23jul = pd.read_csv('konda2023.csv', sep = None, usecols = selected_cols )\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'konda2023.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m selected_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarometre\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1012_s2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s5.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1406_s5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1201_s7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s7.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1201_s29\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1201_s33.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s52\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1301_s53.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1301_s54.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1510_s47\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1012_s58\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1302_s61\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1201_s54\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1502_s22\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s27\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1205_s4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB1003_s4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s14.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s17\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s18.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s20.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s20.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s21.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s21.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s21.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.6\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s26.0.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB2307_s28.8\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m konda_23jul \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkonda2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, usecols \u001b[38;5;241m=\u001b[39m selected_cols )\n\u001b[0;32m      3\u001b[0m konda_23jul \u001b[38;5;241m=\u001b[39m konda_23jul\u001b[38;5;241m.\u001b[39mloc[konda_23jul[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarometre\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m144\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'konda2023.csv'"
     ]
    }
   ],
   "source": [
    "selected_cols = [\"Barometre\",\"B1003_s1\", \"B1012_s2\", \"B1003_s3\", \"B1003_s3.1\", \"B1003_s5\", \"B1003_s5.1\", \"B1003_s8\", \"B1406_s5\", \"B1201_s7\", \"B1003_s7.1\", \"B1201_s29\", \"B1201_s33.1\", \"B1003_s52\", \"B1301_s53.1\", \"B1301_s54.1\", \"B1510_s47\", \"B1012_s58\", \"B1302_s61\", \"B1201_s54\", \"B1502_s22\", \"B1003_s27\", \"B1205_s4\", \"B1003_s4\", \"B2307_s14.0.1\", \"B2307_s14.1\", \"B2307_s14.2\", \"B2307_s14.3\", \"B2307_s14.4\", \"B2307_s14.5\", \"B2307_s14.6\", \"B2307_s14.7\", \"B2307_s15\", \"B2307_s17\", \"B2307_s18.1\", \"B2307_s20.3\", \"B2307_s20.4\", \"B2307_s21.1\", \"B2307_s21.2\", \"B2307_s21.4\", \"B2307_s26.0.1\", \"B2307_s26.0.2\", \"B2307_s26.0.3\", \"B2307_s26.0.4\", \"B2307_s26.0.5\", \"B2307_s26.0.6\",\"B2307_s26.0.7\", \"B2307_s28.1\", \"B2307_s28.2\", \"B2307_s28.3\", \"B2307_s28.4\", \"B2307_s28.5\", \"B2307_s28.6\", \"B2307_s28.7\", \"B2307_s28.8\"]\n",
    "konda_23jul = pd.read_csv('konda2023.csv', sep = None, usecols = selected_cols )\n",
    "konda_23jul = konda_23jul.loc[konda_23jul[\"Barometre\"] == \"144\"].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc959a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "konda_23jul.rename(columns = {\"Bölge\":\"Region\",\"Kırkent\":\"City_rural\",\"B1003_s1\":\"Sex\", \"B1012_s2\":\"Age\", \"B1003_s3\":\"Educ\", \"B1003_s3.1\":\"Educ_grouped\", \"B1003_s5\":\"Birth_place\", \"B1003_s5.1\":\"Birth_placeRegion\", \"B1003_s8\":\"Work\", \"B1406_s5\":\"Growing_place\", \"B1201_s7\":\"Home_pop\", \"B1003_s7.1\":\"Home_popGrouped\", \"B1201_s29\":\"Life_style\", \"B1201_s33.1\":\"Marital\", \"B1003_s52\":\"Purdah\", \"B1301_s53.1\":\"Ethnicity\", \"B1301_s54.1\":\"Religion\", \"B1510_s47\":\"Religiosity\", \"B1012_s58\":\"Income\", \"B1302_s61\":\"Income_PC\", \"B1201_s54\":\"Home_econ\", \"B1502_s22\":\"RTE_note\", \"B1003_s27\":\"Happiness\", \"B1205_s4\":\"Mother_educ\", \"B1003_s4\":\"Father_educ\", \"B2307_s14.0.1\":\"Childhood_fam\", \"B2307_s14.1\":\"Childhood_mom\", \"B2307_s14.2\":\"Childhood_dad\", \"B2307_s14.3\":\"Childhood_sibl\", \"B2307_s14.4\":\"Childhood_grandpar\", \"B2307_s14.5\":\"Childhood_relatives\", \"B2307_s14.6\":\"Childhood_orphanage\", \"B2307_s14.7\":\"Childhood_NA\", \"B2307_s15\":\"Childhood_happiness\", \"B2307_s17\":\"Childhood_age\", \"B2307_s18.1\":\"Childhood_work\", \"B2307_s20.3\":\"Childhood_love\", \"B2307_s20.4\":\"Childhood_apprec\", \"B2307_s21.1\":\"Childhood_familyAttitude\", \"B2307_s21.2\":\"Childhood_popularity\", \"B2307_s21.4\":\"Childhood_familyValue\", \"B2307_s26.0.1\":\"Childhood_discNo\", \"B2307_s26.0.2\":\"Childhood_discEthni\", \"B2307_s26.0.3\":\"Childhood_discRelig\", \"B2307_s26.0.4\":\"Childhood_discLang\", \"B2307_s26.0.5\":\"Childhood_discEcon\", \"B2307_s26.0.6\":\"Childhood_discGender\",\"B2307_s26.0.7\":\"Childhood_discNA\", \"B2307_s28.1\":\"Childhood_viol\", \"B2307_s28.2\":\"Childhood_violMom\", \"B2307_s28.3\":\"Childhood_violDad\", \"B2307_s28.4\":\"Childhood_violSibl\", \"B2307_s28.5\":\"Childhood_violTeach\", \"B2307_s28.6\":\"Childhood_violRels\", \"B2307_s28.7\":\"Childhood_violOther\", \"B2307_s28.8\":\"Childhood_violNA\"}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9725b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "konda_23jul = konda_23jul.apply(pd.to_numeric, errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "konda_23jul.dropna(axis='rows', how='any', inplace=True)\n",
    "konda_23jul = konda_23jul[~konda_23jul.eq(99).any(axis=1)]\n",
    "konda_23jul = konda_23jul[~((konda_23jul['Childhood_NA'] == 1) | (konda_23jul['Childhood_violNA'] == 1) | (konda_23jul[\"Childhood_discNA\"] == 1))]\n",
    "konda_23jul = konda_23jul.drop(['Childhood_NA', 'Childhood_violNA', \"Childhood_discNA\"], axis=1)\n",
    "konda_23jul['Age_group'] = pd.qcut(konda_23jul['Age'], [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], labels=False) + 1\n",
    "konda_23jul['Income_group'] = pd.qcut(konda_23jul['Income'], [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], labels=False) + 1\n",
    "konda_23jul = konda_23jul.apply(pd.to_numeric, errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "konda_23jul.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c72b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b413427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in konda_23jul.columns[:]:\n",
    "    plt.figure(figsize=(16,9))\n",
    "    print(sns.countplot(x = i , data = konda_23jul, palette= \"Set2\" ))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "for i in konda_23jul:\n",
    "    contingency_table = pd.crosstab(konda_23jul['Childhood_viol'], konda_23jul[i])\n",
    "    print(contingency_table)\n",
    "    chi2, p, _, _ = chi2_contingency(pd.crosstab(konda_23jul['Childhood_viol'], konda_23jul[i]))\n",
    "    print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17abe5f3",
   "metadata": {},
   "source": [
    "# Part2  Traditional Variable Selection vs Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = konda_23jul.drop(columns=[\"index\", 'Barometre', 'Childhood_viol', \"Age\", \"Income\", \"Home_pop\", \"Birth_place\", \"Childhood_violMom\",\"Childhood_violDad\",\"Childhood_violSibl\", \"Childhood_violTeach\", \"Childhood_violRels\", \"Childhood_violOther\" ]).reset_index(drop=True)\n",
    "X1 = konda_23jul[[\"Home_popGrouped\", \"Life_style\", \"Purdah\", \"Ethnicity\", \"Religion\", \"Age_group\", \"Income_group\" ]]\n",
    "X2 = konda_23jul[[\"Sex\", \"Educ\", \"Growing_place\", \"Ethnicity\", \"Happiness\", \"Childhood_discEthni\", \"Childhood_discRelig\", \"Childhood_discLang\", \"Childhood_discEcon\", \"Childhood_discGender\"]]\n",
    "X3 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Childhood_age\", \"Age_group\"]]\n",
    "X4 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Age_group\", \"Income_group\"]]\n",
    "X5 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Mother_educ\", \"Father_educ\", \"Educ\"]]\n",
    "X6 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Income_group\", \"Educ\", \"Life_style\", \"Religiosity\", \"Childhood_work\"]]\n",
    "X7 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Income_group\", \"Educ\", \"Life_style\", \"Religiosity\", \"Childhood_age\"]]\n",
    "X8 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Birth_placeRegion\", \"Home_econ\", \"Growing_place\", \"Life_style\", \"Mother_educ\",\"Childhood_age\"]]\n",
    "X9 = konda_23jul[[\"Sex\", \"Ethnicity\", \"Birth_placeRegion\", \"Home_econ\", \"Growing_place\", \"Life_style\", \"Mother_educ\"]]\n",
    "X10 = konda_23jul[[\"Childhood_familyValue\", \"Childhood_popularity\", \"Childhood_familyAttitude\", \"Childhood_familyAttitude\", \"Childhood_apprec\", \"Childhood_love\"]]\n",
    "X11 = konda_23jul[[\"Childhood_relatives\", \"Childhood_grandpar\", \"Childhood_sibl\", \"Childhood_dad\", \"Childhood_mom\", \"Childhood_love\"]]\n",
    "X12 = konda_23jul[[\"Growing_place\", \"Life_style\", \"Ethnicity\", \"Mother_educ\", \"Father_educ\", \"Childhood_fam\"]]\n",
    "X13 = konda_23jul[[\"Sex\", \"Life_style\", \"Ethnicity\", \"Mother_educ\", \"Father_educ\"]]\n",
    "\n",
    "\n",
    "#Assign data from first fifth columns to y variable\n",
    "y1 = konda_23jul[\"Childhood_viol\"].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd695fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X5_train, X5_test, y5_train, y5_test = train_test_split(X5, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X6, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X7_train, X7_test, y7_train, y7_test = train_test_split(X7, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X8_train, X8_test, y8_train, y8_test = train_test_split(X8, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X9_train, X9_test, y9_train, y9_test = train_test_split(X9, y1, test_size = 0.3, random_state=1, stratify=y1)\n",
    "X10_train, X10_test, y10_train, y10_test = train_test_split(X10, y1, test_size = 0.3, random_state=1)\n",
    "X11_train, X11_test, y11_train, y11_test = train_test_split(X11, y1, test_size = 0.3, random_state=1)\n",
    "X12_train, X12_test, y12_train, y12_test = train_test_split(X12, y1, test_size = 0.3, random_state=1)\n",
    "X13_train, X13_test, y13_train, y13_test = train_test_split(X13, y1, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler3 = StandardScaler()\n",
    "scaler4 = StandardScaler()\n",
    "scaler5 = StandardScaler()\n",
    "scaler6 = StandardScaler()\n",
    "scaler7 = StandardScaler()\n",
    "scaler8 = StandardScaler()\n",
    "scaler9 = StandardScaler()\n",
    "scaler10 = StandardScaler()\n",
    "scaler11 = StandardScaler()\n",
    "scaler12 = StandardScaler()\n",
    "scaler13 = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "scaler1.fit(X1_train)\n",
    "scaler2.fit(X2_train)\n",
    "scaler3.fit(X3_train)\n",
    "scaler4.fit(X4_train)\n",
    "scaler5.fit(X5_train)\n",
    "scaler6.fit(X6_train)\n",
    "scaler7.fit(X7_train)\n",
    "scaler8.fit(X8_train)\n",
    "scaler9.fit(X9_train)\n",
    "scaler10.fit(X10_train)\n",
    "scaler11.fit(X11_train)\n",
    "scaler12.fit(X12_train)\n",
    "scaler13.fit(X13_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X1_train = scaler1.transform(X1_train)\n",
    "X1_test = scaler1.transform(X1_test)\n",
    "\n",
    "X2_train = scaler2.transform(X2_train)\n",
    "X2_test = scaler2.transform(X2_test)\n",
    "\n",
    "X3_train = scaler3.transform(X3_train)\n",
    "X3_test = scaler3.transform(X3_test)\n",
    "\n",
    "X4_train = scaler4.transform(X4_train)\n",
    "X4_test = scaler4.transform(X4_test)\n",
    "\n",
    "X5_train = scaler5.transform(X5_train)\n",
    "X5_test = scaler5.transform(X5_test)\n",
    "\n",
    "X6_train = scaler6.transform(X6_train)\n",
    "X6_test = scaler6.transform(X6_test)\n",
    "\n",
    "X7_train = scaler7.transform(X7_train)\n",
    "X7_test = scaler7.transform(X7_test)\n",
    "\n",
    "X8_train = scaler8.transform(X8_train)\n",
    "X8_test = scaler8.transform(X8_test)\n",
    "\n",
    "X9_train = scaler9.transform(X9_train)\n",
    "X9_test = scaler9.transform(X9_test)\n",
    "\n",
    "X10_train = scaler10.transform(X10_train)\n",
    "X10_test = scaler10.transform(X10_test)\n",
    "\n",
    "X11_train = scaler11.transform(X11_train)\n",
    "X11_test = scaler11.transform(X11_test)\n",
    "\n",
    "X12_train = scaler12.transform(X12_train)\n",
    "X12_test = scaler12.transform(X12_test)\n",
    "\n",
    "X13_train = scaler13.transform(X13_train)\n",
    "X13_test = scaler13.transform(X13_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp3 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp4 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp5 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp6 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp7 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp8 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp9 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp10 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp11 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp12 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "mlp13 = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=10000, activation=\"relu\", solver=\"lbfgs\")\n",
    "\n",
    "\n",
    "# default activation fnc. is relu (rectified linear unit)\n",
    "#Comments on alternatives: ‘identity:Best for base X Precision, 0 guess for others, ‘logistic:The worst’, ‘tanh:Best overall’, ‘relu:Good, similar to tanh but worse’\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp1.fit(X1_train, y1_train)\n",
    "mlp2.fit(X2_train, y2_train)\n",
    "mlp3.fit(X3_train, y3_train)\n",
    "mlp4.fit(X4_train, y4_train)\n",
    "mlp5.fit(X5_train, y5_train)\n",
    "mlp6.fit(X6_train, y6_train)\n",
    "mlp7.fit(X7_train, y7_train)\n",
    "mlp8.fit(X8_train, y8_train)\n",
    "mlp9.fit(X9_train, y9_train)\n",
    "mlp10.fit(X10_train, y10_train)\n",
    "mlp11.fit(X11_train, y11_train)\n",
    "mlp12.fit(X12_train, y12_train)\n",
    "mlp13.fit(X13_train, y13_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "predictions1 = mlp1.predict(X1_test)\n",
    "predictions2 = mlp2.predict(X2_test)\n",
    "predictions3 = mlp3.predict(X3_test)\n",
    "predictions4 = mlp4.predict(X4_test)\n",
    "predictions5 = mlp5.predict(X5_test)\n",
    "predictions6 = mlp6.predict(X6_test)\n",
    "predictions7 = mlp7.predict(X7_test)\n",
    "predictions8 = mlp8.predict(X8_test)\n",
    "predictions9 = mlp9.predict(X9_test)\n",
    "predictions10 = mlp10.predict(X10_test)\n",
    "predictions11 = mlp11.predict(X11_test)\n",
    "predictions12 = mlp12.predict(X12_test)\n",
    "predictions13 = mlp13.predict(X13_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1780742",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "print(\"X\", classification_report(y_test,predictions))\n",
    "print(\"X1\", classification_report(y1_test,predictions1))\n",
    "print(\"X2\", classification_report(y2_test,predictions2))\n",
    "print(\"X3\", classification_report(y3_test,predictions3))\n",
    "print(\"X4\", classification_report(y4_test,predictions4))\n",
    "print(\"X5\", classification_report(y5_test,predictions5))\n",
    "print(\"X6\", classification_report(y6_test,predictions6))\n",
    "print(\"X7\", classification_report(y7_test,predictions7))\n",
    "print(\"X8\", classification_report(y8_test,predictions8))\n",
    "print(\"X9\", classification_report(y9_test,predictions9))\n",
    "print(\"X10\", classification_report(y10_test,predictions10))\n",
    "print(\"X11\", classification_report(y11_test,predictions11))\n",
    "print(\"X12\", classification_report(y12_test,predictions12))\n",
    "print(\"X13\", classification_report(y13_test,predictions13))\n",
    "\n",
    "\n",
    "precision_X,recall_X,fscore_X,_ =score(y1_test,predictions,average='micro', labels=[2])\n",
    "metrics_dataX = {\n",
    "    'Variable': ['X'],\n",
    "    'Precision': precision_X,\n",
    "    'Recall': recall_X,\n",
    "    'F1 Score': fscore_X\n",
    "}\n",
    "metrics_df_X = pd.DataFrame(metrics_dataX)\n",
    "\n",
    "\n",
    "\n",
    "precision_X1,recall_X1,fscore_X1,_ =score(y1_test,predictions1,average='micro', labels=[2])\n",
    "metrics_dataX1 = {\n",
    "    'Variable': ['X1'],\n",
    "    'Precision': precision_X1,\n",
    "    'Recall': recall_X1,\n",
    "    'F1 Score': fscore_X1\n",
    "}\n",
    "metrics_df_X1 = pd.DataFrame(metrics_dataX)\n",
    "\n",
    "\n",
    "\n",
    "precision_X2,recall_X2,fscore_X2,_ =score(y2_test,predictions2,average='micro', labels=[2])\n",
    "metrics_dataX2 = {\n",
    "    'Variable': ['X2'],\n",
    "    'Precision': precision_X2,\n",
    "    'Recall': recall_X2,\n",
    "    'F1 Score': fscore_X2}\n",
    "metrics_df_X2 = pd.DataFrame(metrics_dataX2)\n",
    "\n",
    "\n",
    "\n",
    "precision_X3,recall_X3,fscore_X3,_ =score(y3_test,predictions3,average='micro', labels=[2])\n",
    "metrics_dataX3 = {\n",
    "    'Variable': ['X3'],\n",
    "    'Precision': precision_X3,\n",
    "    'Recall': recall_X3,\n",
    "    'F1 Score': fscore_X3\n",
    "}\n",
    "metrics_df_X3 = pd.DataFrame(metrics_dataX3)\n",
    "\n",
    "\n",
    "\n",
    "precision_X4,recall_X4,fscore_X4,_ =score(y4_test,predictions4,average='micro', labels=[2])\n",
    "metrics_dataX4 = {\n",
    "    'Variable': ['X4'],\n",
    "    'Precision': precision_X4,\n",
    "    'Recall': recall_X4,\n",
    "    'F1 Score': fscore_X4\n",
    "}\n",
    "metrics_df_X4 = pd.DataFrame(metrics_dataX4)\n",
    "\n",
    "\n",
    "\n",
    "precision_X5,recall_X5,fscore_X5,_ =score(y5_test,predictions5, average='micro', labels=[2])\n",
    "metrics_dataX5 = {\n",
    "    'Variable': ['X5'],\n",
    "    'Precision': precision_X5,\n",
    "    'Recall': recall_X5,\n",
    "    'F1 Score': fscore_X5\n",
    "}\n",
    "metrics_df_X5 = pd.DataFrame(metrics_dataX5)\n",
    "\n",
    "\n",
    "\n",
    "precision_X6, recall_X6, fscore_X6,_ = score(y6_test, predictions6, average='micro', labels=[2])\n",
    "metrics_dataX6 = {\n",
    "    'Variable': ['X6'],\n",
    "    'Precision': precision_X6,\n",
    "    'Recall': recall_X6,\n",
    "    'F1 Score': fscore_X6\n",
    "}\n",
    "metrics_df_X6 = pd.DataFrame(metrics_dataX6)\n",
    "\n",
    "\n",
    "precision_X7, recall_X7, fscore_X7, _ = score(y7_test, predictions7, average='micro', labels=[2])\n",
    "metrics_dataX7 = {\n",
    "    'Variable': ['X7'],\n",
    "    'Precision': precision_X7,\n",
    "    'Recall': recall_X7,\n",
    "    'F1 Score': fscore_X7\n",
    "}\n",
    "metrics_df_X7 = pd.DataFrame(metrics_dataX7)\n",
    "\n",
    "precision_X8, recall_X8, fscore_X8, _ = score(y8_test, predictions8, average='micro', labels=[2])\n",
    "metrics_dataX8 = {\n",
    "    'Variable': ['X8'],\n",
    "    'Precision': precision_X8,\n",
    "    'Recall': recall_X8,\n",
    "    'F1 Score': fscore_X8\n",
    "}\n",
    "metrics_df_X8 = pd.DataFrame(metrics_dataX8)\n",
    "\n",
    "\n",
    "precision_X9, recall_X9, fscore_X9, _ = score(y9_test, predictions9, average='micro', labels=[2])\n",
    "metrics_dataX9 = {\n",
    "    'Variable': ['X9'],\n",
    "    'Precision': precision_X9,\n",
    "    'Recall': recall_X9,\n",
    "    'F1 Score': fscore_X9\n",
    "}\n",
    "metrics_df_X9 = pd.DataFrame(metrics_dataX9)\n",
    "\n",
    "\n",
    "\n",
    "precision_X10, recall_X10, fscore_X10, _ = score(y10_test, predictions10, average='micro', labels=[2])\n",
    "metrics_dataX10 = {\n",
    "    'Variable': ['X10'],\n",
    "    'Precision': precision_X10,\n",
    "    'Recall': recall_X10,\n",
    "    'F1 Score': fscore_X10\n",
    "}\n",
    "metrics_df_X10 = pd.DataFrame(metrics_dataX10)\n",
    "\n",
    "\n",
    "\n",
    "precision_X11, recall_X11, fscore_X11, _ = score(y11_test, predictions11, average='micro', labels=[2])\n",
    "metrics_dataX11 = {\n",
    "    'Variable': ['X11'],\n",
    "    'Precision': precision_X11,\n",
    "    'Recall': recall_X11,\n",
    "    'F1 Score': fscore_X11\n",
    "}\n",
    "metrics_df_X11 = pd.DataFrame(metrics_dataX11)\n",
    "\n",
    "\n",
    "precision_X12, recall_X12, fscore_X12, _ = score(y12_test, predictions12, average='micro', labels=[2])\n",
    "metrics_dataX12 = {\n",
    "    'Variable': ['X12'],\n",
    "    'Precision': precision_X12,\n",
    "    'Recall': recall_X12,\n",
    "    'F1 Score': fscore_X12\n",
    "}\n",
    "metrics_df_X12 = pd.DataFrame(metrics_dataX12)\n",
    "\n",
    "\n",
    "precision_X13, recall_X13, fscore_X13, _ = score(y13_test, predictions13, average='micro', labels=[2])\n",
    "metrics_dataX13 = {\n",
    "    'Variable': ['X13'],\n",
    "    'Precision': precision_X13,\n",
    "    'Recall': recall_X13,\n",
    "    'F1 Score': fscore_X13\n",
    "}\n",
    "metrics_df_X13 = pd.DataFrame(metrics_dataX13)\n",
    "\n",
    "\n",
    "all_metrics_df = pd.concat([metrics_df_X, metrics_df_X1, metrics_df_X2, metrics_df_X3, metrics_df_X4, metrics_df_X5, metrics_df_X6, metrics_df_X7, metrics_df_X8,metrics_df_X9, metrics_df_X10, metrics_df_X11, metrics_df_X12, metrics_df_X13 ], ignore_index=True)\n",
    "\n",
    "melted_metrics_df = pd.melt(all_metrics_df, id_vars=['Variable'], var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Variable', y='Value', hue='Metric', data=melted_metrics_df, palette='prism')\n",
    "plt.title('Metrics Comparison for Different Variables')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6090c8c",
   "metadata": {},
   "source": [
    "X1 = \"Home_popGrouped\", \"Life_style\", \"Purdah\", \"Ethnicity\", \"Religion\", \"Age_group\", \"Income_group\" \n",
    "\n",
    "X2 = \"Sex\", \"Educ\", \"Growing_place\", \"Ethnicity\", \"Happiness\", \"Childhood_discEthni\", \"Childhood_discRelig\", \"Childhood_discLang\", \"Childhood_discEcon\", \"Childhood_discGender\"\n",
    "\n",
    "X3 = \"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Childhood_age\", \"Age_group\"\n",
    "\n",
    "X4 = \"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Age_group\", \"Income_group\"\n",
    "\n",
    "**X5 = \"Sex\", \"Ethnicity\", \"Birth_place\", \"Growing_place\", \"Mother_educ\", \"Father_educ\", \"Educ\"**\n",
    "\n",
    "**X6 = \"Sex\", \"Ethnicity\", \"Income_group\", \"Educ\", \"Life_style\", \"Religiosity\", \"Childhood_work\"**\n",
    "\n",
    "X7 = \"Sex\", \"Ethnicity\", \"Income_group\", \"Educ\", \"Life_style\", \"Religiosity\", \"Childhood_age\"\n",
    "\n",
    "X8 = \"Sex\", \"Ethnicity\", \"Birth_placeRegion\", \"Home_econ\", \"Growing_place\", \"Life_style\", \"Mother_educ\",\"Childhood_age\"\n",
    "\n",
    "X9 = \"Sex\", \"Ethnicity\", \"Birth_placeRegion\", \"Home_econ\", \"Growing_place\", \"Life_style\", \"Mother_educ\"\n",
    "\n",
    "**X10 = \"Childhood_familyValue\", \"Childhood_popularity\", \"Childhood_familyAttitude\", \"Childhood_familyAttitude\", \"Childhood_apprec\", \"Childhood_love\"**\n",
    "\n",
    "X11 = \"Childhood_relatives\", \"Childhood_grandpar\", \"Childhood_sibl\", \"Childhood_dad\", \"Childhood_mom\", \"Childhood_love\"\n",
    "\n",
    "X12 = \"Growing_place\", \"Life_style\", \"Ethnicity\", \"Mother_educ\", \"Father_educ\", \"Childhood_fam\"\n",
    "\n",
    "X13 = \"Sex\", \"Life_style\", \"Ethnicity\", \"Mother_educ\", \"Father_educ\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc9dfa",
   "metadata": {},
   "source": [
    "## Part3: Optimization by Balancing and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Repository\n",
    "# https://imbalanced-learn.org/stable/over_sampling.html\n",
    "# pip install -U imbalanced-learn\n",
    "\n",
    "# Oversampling is used after doing train-test splits.\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "#Note that all training and random sampling used with the same random state for comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Algorithms used for comparison\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#Preprocessing and Classification Score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Metric Printing to graph\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "\n",
    "\n",
    "X_MLP_train, X_MLP_test, y_MLP_train, y_MLP_test = train_test_split(X, y1, test_size = 0.5, random_state=1, stratify=y1)\n",
    "\n",
    "\n",
    "X_MLP_train, y_MLP_train = ros.fit_resample(X_MLP_train, y_MLP_train)\n",
    "\n",
    "\n",
    "scaler_MLP = StandardScaler()\n",
    "scaler_MLP.fit(X_MLP_train)\n",
    "\n",
    "X_MLP_train = scaler_MLP.transform(X_MLP_train)\n",
    "X_MLP_test = scaler_MLP.transform(X_MLP_test)\n",
    "\n",
    "\n",
    "mlp_resampled = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=1000, random_state=1, activation=\"identity\", solver=\"lbfgs\")\n",
    "mlp_resampled.fit(X_MLP_train, y_MLP_train)\n",
    "predictions_MLP = mlp_resampled.predict(X_MLP_test)\n",
    "\n",
    "\n",
    "print(\"X\", classification_report(y_MLP_test,predictions_MLP))\n",
    "\n",
    "precision_MLP,recall_MLP,fscore_MLP,_ =score(y_MLP_test,predictions_MLP,average='micro', labels=[2])\n",
    "\n",
    "metrics_MLP = {\n",
    "    'Variable': ['MLP'],\n",
    "    'Precision': precision_MLP,\n",
    "    'Recall': recall_MLP,\n",
    "    'F1 Score': fscore_MLP\n",
    "}\n",
    "metrics_MLP = pd.DataFrame(metrics_MLP)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa283821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "X_logreg_train, X_logreg_test, y_logreg_train, y_logreg_test = train_test_split(X, y1, test_size = 0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_logreg_train, y_logreg_train = ros.fit_resample(X_logreg_train, y_logreg_train)\n",
    "\n",
    "scaler_logreg = StandardScaler()\n",
    "scaler_logreg.fit(X_logreg_train)\n",
    "\n",
    "X_logreg_train = scaler_logreg.transform(X_logreg_train)\n",
    "X_logreg_test = scaler_logreg.transform(X_logreg_test)\n",
    "\n",
    "clf_logreg = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=1)\n",
    "clf_logreg.fit(X_logreg_train, y_logreg_train)\n",
    "predictions_logreg = clf_logreg.predict(X_logreg_test)\n",
    "\n",
    "\n",
    "print(\"Logistic Regression\", classification_report(y_logreg_test,predictions_logreg))\n",
    "\n",
    "precision_logreg,recall_logreg,fscore_logreg,_ =score(y_logreg_test,predictions_logreg,average='micro', labels=[2])\n",
    "metrics_LogReg = {\n",
    "    'Variable': ['LogReg'],\n",
    "    'Precision': precision_logreg,\n",
    "    'Recall': recall_logreg,\n",
    "    'F1 Score': fscore_logreg\n",
    "}\n",
    "metrics_LogReg = pd.DataFrame(metrics_LogReg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6019aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K_Nearest Neighbors Classifier\n",
    "\n",
    "\n",
    "X_KNN_train, X_KNN_test, y_KNN_train, y_KNN_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_KNN_train, y_KNN_train = ros.fit_resample(X_KNN_train, y_KNN_train)\n",
    "\n",
    "\n",
    "scaler_KNN = StandardScaler()\n",
    "scaler_KNN.fit(X_KNN_train)\n",
    "\n",
    "X_KNN_train = scaler_KNN.transform(X_KNN_train)\n",
    "X_KNN_test = scaler_KNN.transform(X_KNN_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf_KNN=KNeighborsClassifier(n_neighbors=2)\n",
    "clf_KNN.fit(X_KNN_train,y_KNN_train)\n",
    "predictions_KNN=clf_KNN.predict(X_KNN_test)\n",
    "\n",
    "print(\"K Nearest Neighbors\", classification_report(y_KNN_test,predictions_KNN))\n",
    "\n",
    "precision_KNN,recall_KNN,fscore_KNN,_ =score(y_KNN_test,predictions_KNN,average='micro', labels=[2])\n",
    "metrics_KNN = {\n",
    "    'Variable': ['KNN'],\n",
    "    'Precision': precision_KNN,\n",
    "    'Recall': recall_KNN,\n",
    "    'F1 Score': fscore_KNN\n",
    "}\n",
    "metrics_KNN = pd.DataFrame(metrics_KNN)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent Classifier\n",
    "\n",
    "X_SGD_train, X_SGD_test, y_SGD_train, y_SGD_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_SGD_train, y_SGD_train = ros.fit_resample(X_SGD_train, y_SGD_train)\n",
    "\n",
    "scaler_SGD = StandardScaler()\n",
    "scaler_SGD.fit(X_SGD_train)\n",
    "\n",
    "X_SGD_train = scaler_SGD.transform(X_SGD_train)\n",
    "X_SGD_test = scaler_SGD.transform(X_SGD_test)\n",
    "\n",
    "\n",
    "clf_SGD = SGDClassifier(loss=\"modified_huber\", max_iter=1000, learning_rate=\"optimal\", alpha=0.01)\n",
    "clf_SGD.fit(X_SGD_train, y_SGD_train)\n",
    "predictions_SGD = clf_SGD.predict(X_SGD_test)\n",
    "\n",
    "print(\"Stochastic Gradient Classifier\", classification_report(y_SGD_test,predictions_SGD))\n",
    "\n",
    "precision_SGD,recall_SGD,fscore_SGD,_ =score(y_SGD_test,predictions_SGD,average='micro', labels=[2])\n",
    "metrics_SGD = {\n",
    "    'Variable': ['SGD'],\n",
    "    'Precision': precision_SGD,\n",
    "    'Recall': recall_SGD,\n",
    "    'F1 Score': fscore_SGD\n",
    "}\n",
    "metrics_SGD = pd.DataFrame(metrics_SGD)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "X_RForest_train, X_RForest_test, y_RForest_train, y_RForest_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_RForest_train, y_RForest_train = ros.fit_resample(X_RForest_train, y_RForest_train)\n",
    "\n",
    "\n",
    "clf_RForest = RandomForestClassifier(n_estimators=10000, max_depth=15)\n",
    "clf_RForest.fit(X_RForest_train, y_RForest_train)\n",
    "predictions_RForest = clf_RForest.predict(X_RForest_test)\n",
    "\n",
    "print(\"Random Forest Classifier\", classification_report(y_RForest_test,predictions_RForest))\n",
    "\n",
    "precision_RForest,recall_RForest,fscore_RForest,_ =score(y_RForest_test,predictions_RForest,average='micro', labels=[2])\n",
    "metrics_RForest = {\n",
    "    'Variable': ['RForest'],\n",
    "    'Precision': precision_RForest,\n",
    "    'Recall': recall_RForest,\n",
    "    'F1 Score': fscore_RForest\n",
    "}\n",
    "metrics_RForest = pd.DataFrame(metrics_RForest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "X_DTree_train, X_DTree_test, y_DTree_train, y_DTree_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_DTree_train, y_DTree_train = ros.fit_resample(X_DTree_train, y_DTree_train)\n",
    "\n",
    "clf_DTree = DecisionTreeClassifier(max_depth=20)\n",
    "clf_DTree.fit(X_DTree_train, y_DTree_train)\n",
    "predictions_Dtree = clf_DTree.predict(X_DTree_test)\n",
    "\n",
    "print(\"Decision Tree\", classification_report(y_DTree_test,predictions_Dtree))\n",
    "\n",
    "precision_Dtree,recall_Dtree,fscore_Dtree,_ =score(y_DTree_test,predictions_Dtree,average='micro', labels=[2])\n",
    "metrics_DTree = {\n",
    "    'Variable': ['DTree'],\n",
    "    'Precision': precision_Dtree,\n",
    "    'Recall': recall_Dtree,\n",
    "    'F1 Score': fscore_Dtree\n",
    "}\n",
    "metrics_DTree = pd.DataFrame(metrics_DTree)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "X_SVM_train, X_SVM_test, y_SVM_train, y_SVM_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_SVM_train, y_SVM_train = ros.fit_resample(X_SVM_train, y_SVM_train)\n",
    "\n",
    "scaler_SVM = StandardScaler()\n",
    "scaler_SVM.fit(X_SVM_train)\n",
    "\n",
    "X_SVM_train = scaler_SVM.transform(X_SVM_train)\n",
    "X_SVM_test = scaler_SVM.transform(X_SVM_test)\n",
    "\n",
    "\n",
    "clf_SVM = svm.SVC(gamma='auto', kernel='rbf') \n",
    "clf_SVM.fit(X_SVM_train, y_SVM_train)\n",
    "predictions_SVM = clf_SVM.predict(X_SVM_test)\n",
    "\n",
    "print(\"Support Vector Machines\", classification_report(y_SVM_test,predictions_SVM))\n",
    "\n",
    "precision_SVM,recall_SVM,fscore_SVM,_ = score(y_SVM_test,predictions_SVM,average='micro', labels=[2])\n",
    "metrics_SVM = {\n",
    "    'Variable': ['SVM'],\n",
    "    'Precision': precision_SVM,\n",
    "    'Recall': recall_SVM,\n",
    "    'F1 Score': fscore_SVM\n",
    "}\n",
    "metrics_SVM = pd.DataFrame(metrics_SVM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d478f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Naive Bayes\n",
    "\n",
    "X_MNB_train, X_MNB_test, y_MNB_train, y_MNB_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_MNB_train, y_MNB_train = ros.fit_resample(X_MNB_train, y_MNB_train)\n",
    "\n",
    "clf_MNB = MultinomialNB()\n",
    "clf_MNB.fit(X_MNB_train, y_MNB_train)\n",
    "predictions_MNB = clf_MNB.predict(X_MNB_test)\n",
    "\n",
    "print(\"Categorical Naive Bayes\", classification_report(y_MNB_test,predictions_MNB))\n",
    "\n",
    "\n",
    "precision_MNB,recall_MNB,fscore_MNB,_ = score(y_MNB_test,predictions_MNB,average='micro', labels=[2])\n",
    "\n",
    "metrics_MNB = {\n",
    "    'Variable': ['MNBayes'],\n",
    "    'Precision': precision_MNB,\n",
    "    'Recall': recall_MNB,\n",
    "    'F1 Score': fscore_MNB\n",
    "}\n",
    "metrics_MNB = pd.DataFrame(metrics_MNB)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "X_GBC_train, X_GBC_test, y_GBC_train, y_GBC_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_GBC_train, y_GBC_train = ros.fit_resample(X_GBC_train, y_GBC_train)\n",
    "\n",
    "\n",
    "scaler_GBC = StandardScaler()\n",
    "scaler_GBC.fit(X_GBC_train)\n",
    "\n",
    "X_GBC_train = scaler_GBC.transform(X_GBC_train)\n",
    "X_GBC_test = scaler_GBC.transform(X_GBC_test)\n",
    "\n",
    "\n",
    "clf_GBC = GradientBoostingClassifier(max_depth=7,loss=\"exponential\", n_estimators=1000, learning_rate=0.07)\n",
    "clf_GBC.fit(X_GBC_train, y_GBC_train)\n",
    "predictions_GBC = clf_GBC.predict(X_GBC_test)\n",
    "\n",
    "print(\"Gradient Boosting Classifier\", classification_report(y_GBC_test,predictions_GBC))\n",
    "\n",
    "precision_GBC,recall_GBC,fscore_GBC,_ = score(y_GBC_test,predictions_GBC,average='micro', labels=[2])\n",
    "metrics_GBC = {\n",
    "    'Variable': ['GBC'],\n",
    "    'Precision': precision_GBC,\n",
    "    'Recall': recall_GBC,\n",
    "    'F1 Score': fscore_GBC\n",
    "}\n",
    "metrics_GBC = pd.DataFrame(metrics_GBC)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7454d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost Classifier\n",
    "\n",
    "X_Ada_train, X_Ada_test, y_Ada_train, y_Ada_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_Ada_train, y_Ada_train = ros.fit_resample(X_Ada_train, y_Ada_train)\n",
    "\n",
    "scaler_Ada = StandardScaler()\n",
    "scaler_Ada.fit(X_Ada_train)\n",
    "\n",
    "X_Ada_train = scaler_Ada.transform(X_Ada_train)\n",
    "X_Ada_test = scaler_Ada.transform(X_Ada_test)\n",
    "\n",
    "clf_Ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
    "clf_Ada.fit(X_Ada_train, y_Ada_train)\n",
    "predictions_Ada = clf_Ada.predict(X_Ada_test)\n",
    "\n",
    "print(\"Adaboost Classifier\", classification_report(y_Ada_test,predictions_Ada))\n",
    "\n",
    "precision_Ada,recall_Ada,fscore_Ada,_ = score(y_Ada_test,predictions_Ada,average='micro', labels=[2])\n",
    "metrics_Ada = {\n",
    "    'Variable': ['AdaBst'],\n",
    "    'Precision': precision_Ada,\n",
    "    'Recall': recall_Ada,\n",
    "    'F1 Score': fscore_Ada\n",
    "}\n",
    "metrics_Ada = pd.DataFrame(metrics_Ada)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremely Randomized Trees\n",
    "\n",
    "X_ETrees_train, X_ETrees_test, y_ETrees_train, y_ETrees_test = train_test_split(X, y1, test_size=0.5, random_state=1, stratify=y1)\n",
    "\n",
    "X_ETrees_train, y_ETrees_train = ros.fit_resample(X_ETrees_train, y_ETrees_train)\n",
    "\n",
    "\n",
    "\n",
    "clf_ETrees = ExtraTreesClassifier(n_estimators=100)\n",
    "clf_ETrees.fit(X_ETrees_train, y_ETrees_train)\n",
    "predictions_ETrees = clf_ETrees.predict(X_ETrees_test)\n",
    "\n",
    "\n",
    "print(\"Extremely Randomized Trees\", classification_report(y_ETrees_test,predictions_ETrees))\n",
    "\n",
    "precision_ETrees,recall_ETrees,fscore_ETrees,_ = score(y_ETrees_test,predictions_ETrees,average='micro', labels=[2])\n",
    "metrics_ETrees = {\n",
    "    'Variable': ['ETrees'],\n",
    "    'Precision': precision_ETrees,\n",
    "    'Recall': recall_ETrees,\n",
    "    'F1 Score': fscore_ETrees\n",
    "}\n",
    "metrics_ETrees = pd.DataFrame(metrics_ETrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d90c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_ML = pd.concat([metrics_MLP, metrics_LogReg, metrics_KNN, metrics_SGD, metrics_RForest, metrics_DTree, \n",
    "                            metrics_SVM, metrics_MNB, metrics_GBC,metrics_Ada, metrics_ETrees], ignore_index=True)\n",
    "\n",
    "melted_metrics_ML = pd.melt(all_metrics_ML, id_vars=['Variable'], var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Variable', y='Value', hue='Metric', data=melted_metrics_ML, palette='prism')\n",
    "plt.title('Metrics Comparison for Different Algorithms')\n",
    "plt.xlabel(\"ML Algorithms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfeda1e",
   "metadata": {},
   "source": [
    "## Part4 - Hyperparameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CV, X_test_CV, y_train_CV, y_test_CV = train_test_split(X, y1, test_size = 0.5, random_state=1, stratify=y1)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Define the scoring metric (F1 score for target value 2)\n",
    "f1_scorer = make_scorer(f1_score, pos_label=2)\n",
    "\n",
    "classifiers = {\n",
    "    'MLPClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp_classifier', MLPClassifier())\n",
    "    ]),\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logistic_regression', LogisticRegression())\n",
    "    ]),\n",
    "    'SGDClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('sgd_classifier', SGDClassifier())\n",
    "    ]),\n",
    "    'GradientBoostingClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('gradient_boosting', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'AdaBoostClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('adaboost', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'RandomForestClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('random_forest', RandomForestClassifier())\n",
    "    ]),\n",
    "    'KNeighborsClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('kneighbors', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', svm.SVC(kernel='rbf'))\n",
    "    ]),\n",
    "    'MultinomialNB': Pipeline([\n",
    "        ('multinomial_nb', MultinomialNB())\n",
    "    ]),\n",
    "    'DecisionTreeClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('decision_tree', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'ExtraTreesClassifier': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('extra_trees', ExtraTreesClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grids for each classifier\n",
    "hyperparameter_grids = {\n",
    "    'MLPClassifier': {\n",
    "        'mlp_classifier__hidden_layer_sizes': [(100,), (100, 100, 100)],\n",
    "        'mlp_classifier__max_iter': [1000, 2000, 5000],\n",
    "        'mlp_classifier__mlp_classifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'mlp_classifier__solver': ['lbfgs', 'sgd', 'adam']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'logistic_regression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'logistic_regression__penalty': ['l1', 'l2']\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'sgd_classifier__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        'sgd_classifier__max_iter': [100, 200, 500],\n",
    "        'sgd_classifier__loss': ['hinge', 'log', 'modified_huber']\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'gradient_boosting__n_estimators': [50, 100, 200],\n",
    "        'gradient_boosting__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'AdaBoostClassifier': {\n",
    "        'adaboost__n_estimators': [50, 100, 200],\n",
    "        'adaboost__learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'random_forest__n_estimators': [50, 100, 200],\n",
    "        'random_forest__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighbors__n_neighbors': [3, 5, 7],\n",
    "        'kneighbors__weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'svm__C': [0.1, 1, 10],\n",
    "        'svm__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        'multinomial_nb__alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "        'multinomial_nb__fit_prior': [True, False],\n",
    "        'multinomial_nb__class_prior': [None, [0.7, 0.3]],\n",
    "        'multinomial_nb__norm': [True, False]\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'decision_tree__max_depth': [None, 10, 20],\n",
    "        'decision_tree__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'ExtraTreesClassifier': {\n",
    "        'extra_trees__n_estimators': [50, 100, 200],\n",
    "        'extra_trees__max_depth': [None, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a List to store results\n",
    "results_list = []\n",
    "\n",
    "# Loop through classifiers and perform hyperparameter tuning\n",
    "for name, classifier in classifiers.items():\n",
    "    # Create GridSearchCV instance for each classifier\n",
    "    grid_search = GridSearchCV(classifier, hyperparameter_grids[name], cv=5, scoring=f1_scorer, return_train_score=True)\n",
    "\n",
    "    # Fit GridSearchCV on the training set (X_train and y_train)\n",
    "    grid_search.fit(X_train_CV, y_train_CV)\n",
    "\n",
    "    # Retrieve best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Evaluate the best model on the training set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_train_pred = best_model.predict(X_train_CV)\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_test_pred = best_model.predict(X_test_CV)\n",
    "\n",
    "    # Store results in the List\n",
    "    results_list.append({\n",
    "        'Classifier': name,\n",
    "        'Hyperparameters': best_params,\n",
    "        'Train F1 Score': f1_score(y_train_CV, y_train_pred, pos_label=2),\n",
    "        'Test F1 Score': f1_score(y_test_CV, y_test_pred, pos_label=2)\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(x=[str(params) for params in grid_search.cv_results_['params']], y=grid_search.cv_results_['mean_test_score'], label='Test F1 Score')\n",
    "    sns.lineplot(x=[str(params) for params in grid_search.cv_results_['params']], y=grid_search.cv_results_['mean_train_score'], label='Train F1 Score')\n",
    "    plt.title(f'Hyperparameter Tuning for {name}')\n",
    "    plt.xlabel('Hyperparameters')\n",
    "    plt.ylabel('Mean F1 Score (CV)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Create DataFrame from the list of results\n",
    "results_df = pd.DataFrame(results_list)\n",
    "# Visualize the results using bar plots\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "sns.barplot(x='Train F1 Score', y='Classifier', data=results_df, color='blue', label='Train F1 Score')\n",
    "sns.barplot(x='Test F1 Score', y='Classifier', data=results_df, color='orange', label='Test F1 Score')\n",
    "plt.title('F1 Score for Target Value 2 - Hyperparameter Tuning Results')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Classifier')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_importance = clf_RForest.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, feature_importance)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest - Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe8342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4ddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
